{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Geriatrics_LSTM_implementation.ipynb",
      "provenance": [],
      "mount_file_id": "18TIAOPf5JICGrwnF94evx0HqWOAV0qAR",
      "authorship_tag": "ABX9TyNec5ahnjB9hHejL9NWPsVp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oyquSuMJaxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f4b5db75-38a0-4a30-8509-26903a09c5f7"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 15 µs, sys: 3 µs, total: 18 µs\n",
            "Wall time: 21.7 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM50TW1-tGxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eb3b8845-2c35-4647-8ae2-52b7d0b1554f"
      },
      "source": [
        "% cd /content/\n",
        "if not os.path.isdir(\"geriatrics\"):\n",
        "  % mkdir geriatrics\n",
        "\n",
        "% cd /content/geriatrics/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/geriatrics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reBWIlNntJus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "91804370-8eee-470c-8884-d503f4729ee2"
      },
      "source": [
        "% cd /content/geriatrics\n",
        "if os.path.isdir(\"Geriatrics_Data\"):\n",
        "  print(\"Geriatrics_Data already Exists\")\n",
        "else:\n",
        "  ! git clone https://github.com/saahil-jain/Geriatrics_Data.git\n",
        "  % cd Geriatrics_Data\n",
        "  ! git pull\n",
        "  % cd /content/\n",
        "\n",
        "% cd /content/geriatrics/\n",
        "# checking if pwd is geriatrics_code dir\n",
        "if os.getcwd().split(\"/\")[-1] == \"Geriatrics_Code\":\n",
        "  print(\"Current working directory is already Geriatrics_Code\")\n",
        "elif os.path.isdir(\"Geriatrics_Code\"):\n",
        "  print(\"Geriatrics_Code already Exists\")\n",
        "else:\n",
        "  ! git clone https://github.com/saahil-jain/Geriatrics_Code.git\n",
        "\n",
        "if os.path.isdir(\"Geriatrics_Code\"):\n",
        "  % cd Geriatrics_Code\n",
        "! git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics\n",
            "Geriatrics_Data already Exists\n",
            "/content/geriatrics\n",
            "Geriatrics_Code already Exists\n",
            "/content/geriatrics/Geriatrics_Code\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNJcgb5LtMH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "70ab3b7f-3faa-4111-a99e-8f4c21d8e0a9"
      },
      "source": [
        "%cd /content/geriatrics/Geriatrics_Data/\n",
        "if not os.path.isdir(\"test\"):\n",
        "  % mkdir test\n",
        "  % cd test\n",
        "  % mkdir Video_Fall\n",
        "  % mkdir Video_Regular_Activity\n",
        "\n",
        "  % cd '/content/geriatrics/Geriatrics_Data/Video/Video_Fall/'\n",
        "  % mv fall-10-cam0.mp4 /content/geriatrics/Geriatrics_Data/test/Video_Fall/\n",
        "  % mv fall-27-cam0.mp4 /content/geriatrics/Geriatrics_Data/test/Video_Fall/\n",
        "\n",
        "  % cd '/content/geriatrics/Geriatrics_Data/Video/Video_Regular_Activity/'\n",
        "  % mv adl-10-cam0.mp4 /content/geriatrics/Geriatrics_Data/test/Video_Regular_Activity/\n",
        "  % mv adl-23-cam0.mp4 /content/geriatrics/Geriatrics_Data/test/Video_Regular_Activity/\n",
        "\n",
        "% cd /content/geriatrics/Geriatrics_Data\n",
        "% mv Video train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics/Geriatrics_Data\n",
            "/content/geriatrics/Geriatrics_Data\n",
            "mv: cannot stat 'Video': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Otr-SPYtR9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f4f0ff3-e6d9-435a-d857-96fb9420e3ad"
      },
      "source": [
        "\"\"\"\n",
        "After moving all the files using the 1_ file, we run this one to extract\n",
        "the images from the videos and also create a data file we can use\n",
        "for training and testing later.\n",
        "\"\"\"\n",
        "% cd /content/geriatrics/Geriatrics_Data\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "from subprocess import call\n",
        "\n",
        "def extract_files(extension='mp4'):\n",
        "    \"\"\"After we have all of our videos split between train and test, and\n",
        "    all nested within folders representing their classes, we need to\n",
        "    make a data file that we can reference when training our RNN(s).\n",
        "    This will let us keep track of image sequences and other parts\n",
        "    of the training process.\n",
        "    We'll first need to extract images from each of the videos. We'll\n",
        "    need to record the following data in the file:\n",
        "    [train|test], class, filename, nb frames\n",
        "    Extracting can be done with ffmpeg:\n",
        "    `ffmpeg -i video.mpg image-%04d.jpg`\n",
        "    \"\"\"\n",
        "    data_file = []\n",
        "    folders = ['train', 'test']\n",
        "\n",
        "    for folder in folders:\n",
        "        class_folders = glob.glob(os.path.join(folder, '*'))\n",
        "\n",
        "        for vid_class in class_folders:\n",
        "            class_files = glob.glob(os.path.join(vid_class, '*.' + extension))\n",
        "\n",
        "            for video_path in class_files:\n",
        "                # Get the parts of the file.\n",
        "                video_parts = get_video_parts(video_path)\n",
        "\n",
        "                train_or_test, classname, filename_no_ext, filename = video_parts\n",
        "\n",
        "                # Only extract if we haven't done it yet. Otherwise, just get\n",
        "                # the info.\n",
        "                if not check_already_extracted(video_parts):\n",
        "                    # Now extract it.\n",
        "                    src = os.path.join(train_or_test, classname, filename)\n",
        "                    dest = os.path.join(train_or_test, classname,\n",
        "                        filename_no_ext + '-%04d.jpg')\n",
        "                    call([\"ffmpeg\", \"-i\", src, dest])\n",
        "\n",
        "                # Now get how many frames it is.\n",
        "                nb_frames = get_nb_frames_for_video(video_parts)\n",
        "\n",
        "                data_file.append([train_or_test, classname, filename_no_ext, nb_frames])\n",
        "\n",
        "                print(\"Generated %d frames for %s\" % (nb_frames, filename_no_ext))\n",
        "\n",
        "    with open('data_file.csv', 'w') as fout:\n",
        "        writer = csv.writer(fout)\n",
        "        writer.writerows(data_file)\n",
        "\n",
        "    print(\"Extracted and wrote %d video files.\" % (len(data_file)))\n",
        "\n",
        "def get_nb_frames_for_video(video_parts):\n",
        "    \"\"\"Given video parts of an (assumed) already extracted video, return\n",
        "    the number of frames that were extracted.\"\"\"\n",
        "    train_or_test, classname, filename_no_ext, _ = video_parts\n",
        "    generated_files = glob.glob(os.path.join(train_or_test, classname, filename_no_ext + '*.jpg'))\n",
        "    return len(generated_files)\n",
        "\n",
        "def get_video_parts(video_path):\n",
        "    \"\"\"Given a full path to a video, return its parts.\"\"\"\n",
        "    parts = video_path.split(os.path.sep)\n",
        "    filename = parts[2]\n",
        "    filename_no_ext = filename.split('.')[0]\n",
        "    classname = parts[1]\n",
        "    train_or_test = parts[0]\n",
        "\n",
        "    return train_or_test, classname, filename_no_ext, filename\n",
        "\n",
        "def check_already_extracted(video_parts):\n",
        "    \"\"\"Check to see if we created the -0001 frame of this file.\"\"\"\n",
        "    train_or_test, classname, filename_no_ext, _ = video_parts\n",
        "    return bool(os.path.exists(os.path.join(train_or_test, classname, filename_no_ext + '-0001.jpg')))\n",
        "\n",
        "extract_files()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics/Geriatrics_Data\n",
            "Generated 280 frames for adl-35-cam0\n",
            "Generated 70 frames for adl-24-cam0\n",
            "Generated 125 frames for adl-29-cam0\n",
            "Generated 180 frames for adl-08-cam0\n",
            "Generated 110 frames for adl-25-cam0\n",
            "Generated 100 frames for adl-27-cam0\n",
            "Generated 240 frames for adl-16-cam0\n",
            "Generated 270 frames for adl-20-cam0\n",
            "Generated 265 frames for adl-13-cam0\n",
            "Generated 265 frames for adl-18-cam0\n",
            "Generated 250 frames for adl-19-cam0\n",
            "Generated 230 frames for adl-17-cam0\n",
            "Generated 180 frames for adl-05-cam0\n",
            "Generated 340 frames for adl-36-cam0\n",
            "Generated 235 frames for adl-14-cam0\n",
            "Generated 150 frames for adl-09-cam0\n",
            "Generated 280 frames for adl-21-cam0\n",
            "Generated 270 frames for adl-39-cam0\n",
            "Generated 230 frames for adl-06-cam0\n",
            "Generated 180 frames for adl-02-cam0\n",
            "Generated 345 frames for adl-38-cam0\n",
            "Generated 275 frames for adl-15-cam0\n",
            "Generated 180 frames for adl-07-cam0\n",
            "Generated 400 frames for adl-30-cam0\n",
            "Generated 95 frames for adl-26-cam0\n",
            "Generated 250 frames for adl-31-cam0\n",
            "Generated 300 frames for adl-11-cam0\n",
            "Generated 250 frames for adl-12-cam0\n",
            "Generated 150 frames for adl-04-cam0\n",
            "Generated 150 frames for adl-01-cam0\n",
            "Generated 180 frames for adl-03-cam0\n",
            "Generated 200 frames for adl-32-cam0\n",
            "Generated 350 frames for adl-37-cam0\n",
            "Generated 85 frames for adl-28-cam0\n",
            "Generated 200 frames for adl-33-cam0\n",
            "Generated 330 frames for adl-40-cam0\n",
            "Generated 191 frames for adl-34-cam0\n",
            "Generated 240 frames for adl-22-cam0\n",
            "Generated 60 frames for fall-24-cam0\n",
            "Generated 66 frames for fall-28-cam0\n",
            "Generated 61 frames for fall-26-cam0\n",
            "Generated 61 frames for fall-14-cam0\n",
            "Generated 100 frames for fall-19-cam0\n",
            "Generated 151 frames for fall-05-cam0\n",
            "Generated 70 frames for fall-30-cam0\n",
            "Generated 96 frames for fall-04-cam0\n",
            "Generated 55 frames for fall-21-cam0\n",
            "Generated 99 frames for fall-29-cam0\n",
            "Generated 56 frames for fall-22-cam0\n",
            "Generated 130 frames for fall-11-cam0\n",
            "Generated 75 frames for fall-23-cam0\n",
            "Generated 85 frames for fall-13-cam0\n",
            "Generated 55 frames for fall-16-cam0\n",
            "Generated 100 frames for fall-06-cam0\n",
            "Generated 110 frames for fall-02-cam0\n",
            "Generated 160 frames for fall-01-cam0\n",
            "Generated 185 frames for fall-09-cam0\n",
            "Generated 95 frames for fall-17-cam0\n",
            "Generated 215 frames for fall-03-cam0\n",
            "Generated 110 frames for fall-12-cam0\n",
            "Generated 91 frames for fall-08-cam0\n",
            "Generated 110 frames for fall-20-cam0\n",
            "Generated 65 frames for fall-18-cam0\n",
            "Generated 71 frames for fall-15-cam0\n",
            "Generated 85 frames for fall-25-cam0\n",
            "Generated 156 frames for fall-07-cam0\n",
            "Generated 220 frames for adl-23-cam0\n",
            "Generated 300 frames for adl-10-cam0\n",
            "Generated 92 frames for fall-27-cam0\n",
            "Generated 130 frames for fall-10-cam0\n",
            "Extracted and wrote 70 video files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYg5kqUItVjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d25fc590-4f45-4f53-b65f-0ce39ece29aa"
      },
      "source": [
        "% cd /content/geriatrics/Geriatrics_Data\n",
        "\n",
        "import cv2\n",
        "%cd /content/geriatrics/Geriatrics_Data/train/Video_Fall/\n",
        "\n",
        "im = cv2.imread('fall-01-cam0-0015.jpg')\n",
        "im.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics/Geriatrics_Data\n",
            "/content/geriatrics/Geriatrics_Data/train/Video_Fall\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240, 640, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpOPWeTtYjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7aaf4ecc-6691-4b1d-bf48-03d62164ec71"
      },
      "source": [
        "\"\"\"\n",
        "Class for managing our data.\n",
        "\"\"\"\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import os.path\n",
        "import sys\n",
        "import operator\n",
        "import threading\n",
        "# from processor import process_image\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Process an image that we can pass to our networks.\n",
        "\"\"\"\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "\n",
        "def my_process_image(image, target_shape):\n",
        "    \"\"\"Given an image, process it and return the array.\"\"\"\n",
        "    # Load the image.\n",
        "    h, w, _ = target_shape\n",
        "    image = load_img(image, target_size=(h, w))\n",
        "\n",
        "    # Turn it into numpy, normalize and return.\n",
        "    img_arr = img_to_array(image)\n",
        "    x = (img_arr / 255.).astype(np.float32)\n",
        "\n",
        "    return x\n",
        "\n",
        "class threadsafe_iterator:\n",
        "    def __init__(self, iterator):\n",
        "        self.iterator = iterator\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        with self.lock:\n",
        "            return next(self.iterator)\n",
        "\n",
        "def threadsafe_generator(func):\n",
        "    \"\"\"Decorator\"\"\"\n",
        "    def gen(*a, **kw):\n",
        "        return threadsafe_iterator(func(*a, **kw))\n",
        "    return gen\n",
        "\n",
        "class DataSet():\n",
        "\n",
        "    def __init__(self, seq_length=40, class_limit=None, image_shape=(299, 299, 3)):\n",
        "        \"\"\"Constructor.\n",
        "        seq_length = (int) the number of frames to consider\n",
        "        class_limit = (int) number of classes to limit the data to.\n",
        "            None = no limit.\n",
        "        \"\"\"\n",
        "        self.seq_length = seq_length\n",
        "        self.class_limit = class_limit\n",
        "        self.sequence_path = os.path.join('Geriatrics_Data', 'sequences')\n",
        "        self.max_frames = 300  # max number of frames a video can have for us to use it\n",
        "\n",
        "        # Get the data.\n",
        "        self.data = self.get_data()\n",
        "\n",
        "        # Get the classes.\n",
        "        self.classes = self.get_classes()\n",
        "\n",
        "        # Now do some minor data cleaning.\n",
        "        self.data = self.clean_data()\n",
        "\n",
        "        self.image_shape = image_shape\n",
        "\n",
        "    @staticmethod\n",
        "    def get_data():\n",
        "        \"\"\"Load our data from file.\"\"\"\n",
        "        with open(os.path.join('Geriatrics_Data', 'data_file.csv'), 'r') as fin:\n",
        "            reader = csv.reader(fin)\n",
        "            data = list(reader)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"Limit samples to greater than the sequence length and fewer\n",
        "        than N frames. Also limit it to classes we want to use.\"\"\"\n",
        "        data_clean = []\n",
        "        for item in self.data:\n",
        "            if int(item[3]) >= self.seq_length and int(item[3]) <= self.max_frames \\\n",
        "                    and item[1] in self.classes:\n",
        "                data_clean.append(item)\n",
        "\n",
        "        return data_clean\n",
        "\n",
        "    def get_classes(self):\n",
        "        \"\"\"Extract the classes from our data. If we want to limit them,\n",
        "        only return the classes we need.\"\"\"\n",
        "        classes = []\n",
        "        for item in self.data:\n",
        "            if item[1] not in classes:\n",
        "                classes.append(item[1])\n",
        "\n",
        "        # Sort them.\n",
        "        classes = sorted(classes)\n",
        "\n",
        "        # Return.\n",
        "        if self.class_limit is not None:\n",
        "            return classes[:self.class_limit]\n",
        "        else:\n",
        "            return classes\n",
        "\n",
        "    def get_class_one_hot(self, class_str):\n",
        "        \"\"\"Given a class as a string, return its number in the classes\n",
        "        list. This lets us encode and one-hot it for training.\"\"\"\n",
        "        # Encode it first.\n",
        "        label_encoded = self.classes.index(class_str)\n",
        "\n",
        "        # Now one-hot it.\n",
        "        label_hot = to_categorical(label_encoded, len(self.classes))\n",
        "\n",
        "        assert len(label_hot) == len(self.classes)\n",
        "\n",
        "        return label_hot\n",
        "\n",
        "    def split_train_test(self):\n",
        "        \"\"\"Split the data into train and test groups.\"\"\"\n",
        "        train = []\n",
        "        test = []\n",
        "        for item in self.data:\n",
        "            if item[0] == 'train':\n",
        "                train.append(item)\n",
        "            else:\n",
        "                test.append(item)\n",
        "        return train, test\n",
        "\n",
        "    def get_all_sequences_in_memory(self, train_test, data_type):\n",
        "        \"\"\"\n",
        "        This is a mirror of our generator, but attempts to load everything into\n",
        "        memory so we can train way faster.\n",
        "        \"\"\"\n",
        "        # Get the right dataset.\n",
        "        train, test = self.split_train_test()\n",
        "        data = train if train_test == 'train' else test\n",
        "\n",
        "        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n",
        "\n",
        "        X, y = [], []\n",
        "        for row in data:\n",
        "\n",
        "            if data_type == 'images':\n",
        "                frames = self.get_frames_for_sample(row)\n",
        "                frames = self.rescale_list(frames, self.seq_length)\n",
        "\n",
        "                # Build the image sequence\n",
        "                sequence = self.build_image_sequence(frames)\n",
        "\n",
        "            else:\n",
        "                sequence = self.get_extracted_sequence(data_type, row)\n",
        "\n",
        "                if sequence is None:\n",
        "                    print(\"Can't find sequence. Did you generate them?\")\n",
        "                    raise\n",
        "\n",
        "            X.append(sequence)\n",
        "            y.append(self.get_class_one_hot(row[1]))\n",
        "\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    @threadsafe_generator\n",
        "    def frame_generator(self, batch_size, train_test, data_type):\n",
        "        \"\"\"Return a generator that we can use to train on. There are\n",
        "        a couple different things we can return:\n",
        "        data_type: 'features', 'images'\n",
        "        \"\"\"\n",
        "        # Get the right dataset for the generator.\n",
        "        train, test = self.split_train_test()\n",
        "        data = train if train_test == 'train' else test\n",
        "\n",
        "        print(\"Creating %s generator with %d samples.\" % (train_test, len(data)))\n",
        "\n",
        "        while 1:\n",
        "            X, y = [], []\n",
        "\n",
        "            # Generate batch_size samples.\n",
        "            for _ in range(batch_size):\n",
        "                # Reset to be safe.\n",
        "                sequence = None\n",
        "\n",
        "                # Get a random sample.\n",
        "                sample = random.choice(data)\n",
        "\n",
        "                # Check to see if we've already saved this sequence.\n",
        "                if data_type is \"images\":\n",
        "                    # Get and resample frames.\n",
        "                    frames = self.get_frames_for_sample(sample)\n",
        "                    frames = self.rescale_list(frames, self.seq_length)\n",
        "\n",
        "                    # Build the image sequence\n",
        "                    sequence = self.build_image_sequence(frames)\n",
        "                else:\n",
        "                    # Get the sequence from disk.\n",
        "                    sequence = self.get_extracted_sequence(data_type, sample)\n",
        "\n",
        "                    if sequence is None:\n",
        "                        raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
        "\n",
        "                X.append(sequence)\n",
        "                y.append(self.get_class_one_hot(sample[1]))\n",
        "\n",
        "            yield np.array(X), np.array(y)\n",
        "\n",
        "    def build_image_sequence(self, frames):\n",
        "        \"\"\"Given a set of frames (filenames), build our sequence.\"\"\"\n",
        "        return [my_process_image(x, self.image_shape) for x in frames]\n",
        "\n",
        "    def get_extracted_sequence(self, data_type, sample):\n",
        "        \"\"\"Get the saved extracted features.\"\"\"\n",
        "        filename = sample[2]\n",
        "        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n",
        "            '-' + data_type + '.npy')\n",
        "        if os.path.isfile(path):\n",
        "            return np.load(path)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_frames_by_filename(self, filename, data_type):\n",
        "        \"\"\"Given a filename for one of our samples, return the data\n",
        "        the model needs to make predictions.\"\"\"\n",
        "        # First, find the sample row.\n",
        "        sample = None\n",
        "        for row in self.data:\n",
        "            if row[2] == filename:\n",
        "                sample = row\n",
        "                break\n",
        "        if sample is None:\n",
        "            raise ValueError(\"Couldn't find sample: %s\" % filename)\n",
        "\n",
        "        if data_type == \"images\":\n",
        "            # Get and resample frames.\n",
        "            frames = self.get_frames_for_sample(sample)\n",
        "            frames = self.rescale_list(frames, self.seq_length)\n",
        "            # Build the image sequence\n",
        "            sequence = self.build_image_sequence(frames)\n",
        "        else:\n",
        "            # Get the sequence from disk.\n",
        "            sequence = self.get_extracted_sequence(data_type, sample)\n",
        "\n",
        "            if sequence is None:\n",
        "                raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    @staticmethod\n",
        "    def get_frames_for_sample(sample):\n",
        "        \"\"\"Given a sample row from the data file, get all the corresponding frame\n",
        "        filenames.\"\"\"\n",
        "        path = os.path.join('Geriatrics_Data', sample[0], sample[1])\n",
        "        filename = sample[2]\n",
        "        images = sorted(glob.glob(os.path.join(path, filename + '*jpg')))\n",
        "        return images\n",
        "\n",
        "    @staticmethod\n",
        "    def get_filename_from_image(filename):\n",
        "        parts = filename.split(os.path.sep)\n",
        "        return parts[-1].replace('.jpg', '')\n",
        "\n",
        "    @staticmethod\n",
        "    def rescale_list(input_list, size):\n",
        "        \"\"\"Given a list and a size, return a rescaled/samples list. For example,\n",
        "        if we want a list of size 5 and we have a list of size 25, return a new\n",
        "        list of size five which is every 5th element of the origina list.\"\"\"\n",
        "        assert len(input_list) >= size\n",
        "\n",
        "        # Get the number to skip between iterations.\n",
        "        skip = len(input_list) // size\n",
        "\n",
        "        # Build our new output.\n",
        "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
        "\n",
        "        # Cut off the last one if needed.\n",
        "        return output[:size]\n",
        "\n",
        "    def print_class_from_prediction(self, predictions, nb_to_return=5):\n",
        "        \"\"\"Given a prediction, print the top classes.\"\"\"\n",
        "        # Get the prediction for each label.\n",
        "        label_predictions = {}\n",
        "        for i, label in enumerate(self.classes):\n",
        "            label_predictions[label] = predictions[i]\n",
        "\n",
        "        # Now sort them.\n",
        "        sorted_lps = sorted(\n",
        "            label_predictions.items(),\n",
        "            key=operator.itemgetter(1),\n",
        "            reverse=True\n",
        "        )\n",
        "        result = []\n",
        "        # And return the top N.\n",
        "        for i, class_prediction in enumerate(sorted_lps):\n",
        "            if i > nb_to_return - 1 or class_prediction[1] == 0.0:\n",
        "                break\n",
        "            print(\"%s: %.2f\" % (class_prediction[0], class_prediction[1]))\n",
        "            result.append(\"%s: %.2f\" % (class_prediction[0], class_prediction[1]))\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NJnclvntkQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "import numpy as np\n",
        "\n",
        "class Extractor():\n",
        "    def __init__(self, image_shape=(299, 299, 3), weights=None):\n",
        "        \"\"\"Either load pretrained from imagenet, or load our saved\n",
        "        weights from our own training.\"\"\"\n",
        "\n",
        "        self.weights = weights  # so we can check elsewhere which model\n",
        "\n",
        "        input_tensor = Input(image_shape)\n",
        "        # Get model with pretrained weights.\n",
        "        base_model = InceptionV3(\n",
        "            input_tensor=input_tensor,\n",
        "            weights='imagenet',\n",
        "            include_top=True\n",
        "        )\n",
        "\n",
        "        # We'll extract features at the final pool layer.\n",
        "        self.model = Model(\n",
        "            inputs=base_model.input,\n",
        "            outputs=base_model.get_layer('avg_pool').output\n",
        "        )\n",
        "\n",
        "    def extract(self, image_path):\n",
        "        img = load_img(image_path)\n",
        "\n",
        "        return self.extract_image(img)\n",
        "\n",
        "    def extract_image(self, img):\n",
        "        x = img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "\n",
        "        # Get the prediction.\n",
        "        features = self.model.predict(x)\n",
        "\n",
        "        if self.weights is None:\n",
        "            # For imagenet/default network:\n",
        "            features = features[0]\n",
        "        else:\n",
        "            # For loaded network:\n",
        "            features = features[0]\n",
        "\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw2sSIYNtnDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This script generates extracted features for each video, which other\n",
        "models make use of.\n",
        "You can change you sequence length and limit to a set number of classes\n",
        "below.\n",
        "class_limit is an integer that denotes the first N classes you want to\n",
        "extract features from.\n",
        "Then set the same number when training models.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import os.path\n",
        "# from data import DataSet\n",
        "# from extractor import Extractor\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(seq_length=40, class_limit=2, image_shape=(299, 299, 3)):\n",
        "    # Get the dataset.\n",
        "    data = DataSet(seq_length=seq_length, class_limit=class_limit, image_shape=image_shape)\n",
        "\n",
        "    # get the model.\n",
        "    model = Extractor(image_shape=image_shape)\n",
        "\n",
        "    # Loop through data.\n",
        "    pbar = tqdm(total=len(data.data))\n",
        "    for video in data.data:\n",
        "\n",
        "        # Get the path to the sequence for this video.\n",
        "        path = os.path.join('Geriatrics_Data', 'sequences', video[2] + '-' + str(seq_length) + \\\n",
        "            '-features')  # numpy will auto-append .npy\n",
        "\n",
        "        # Check if we already have it.\n",
        "        if os.path.isfile(path + '.npy'):\n",
        "            pbar.update(1)\n",
        "            continue\n",
        "\n",
        "        # Get the frames for this video.\n",
        "        frames = data.get_frames_for_sample(video)\n",
        "\n",
        "        # Now downsample to just the ones we need.\n",
        "        frames = data.rescale_list(frames, seq_length)\n",
        "\n",
        "        # Now loop through and extract features to build the sequence.\n",
        "        sequence = []\n",
        "        for image in frames:\n",
        "            features = model.extract(image)\n",
        "            sequence.append(features)\n",
        "\n",
        "        # Save the sequence.\n",
        "        np.save(path, sequence)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn3N2qIqtpX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "A collection of models we'll use to attempt to classify videos.\n",
        "\"\"\"\n",
        "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D,\n",
        "    MaxPooling2D)\n",
        "from collections import deque\n",
        "import sys\n",
        "\n",
        "class ResearchModels():\n",
        "    def __init__(self, nb_classes, model, seq_length,\n",
        "                 saved_model=None, features_length=2048):\n",
        "        \"\"\"\n",
        "        `model` = lstm (only one for this case)\n",
        "        `nb_classes` = the number of classes to predict\n",
        "        `seq_length` = the length of our video sequences\n",
        "        `saved_model` = the path to a saved Keras model to load\n",
        "        \"\"\"\n",
        "\n",
        "        # Set defaults.\n",
        "        self.seq_length = seq_length\n",
        "        self.load_model = load_model\n",
        "        self.saved_model = saved_model\n",
        "        self.nb_classes = nb_classes\n",
        "        self.feature_queue = deque()\n",
        "\n",
        "        # Set the metrics. Only use top k if there's a need.\n",
        "        metrics = ['accuracy']\n",
        "        if self.nb_classes >= 10:\n",
        "            metrics.append('top_k_categorical_accuracy')\n",
        "\n",
        "        # Get the appropriate model.\n",
        "        if self.saved_model is not None:\n",
        "            print(\"Loading model %s\" % self.saved_model)\n",
        "            self.model = load_model(self.saved_model)\n",
        "        elif model == 'lstm':\n",
        "            print(\"Loading LSTM model.\")\n",
        "            self.input_shape = (seq_length, features_length)\n",
        "            self.model = self.lstm()\n",
        "        else:\n",
        "            print(\"Unknown network.\")\n",
        "            sys.exit()\n",
        "\n",
        "        # Now compile the network.\n",
        "        optimizer = Adam(lr=1e-5, decay=1e-6)\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "                           metrics=metrics)\n",
        "\n",
        "        print(self.model.summary())\n",
        "\n",
        "    def lstm(self):\n",
        "        \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
        "        our CNN to this model predomenently.\"\"\"\n",
        "        # Model.\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(2048, return_sequences=False,\n",
        "                       input_shape=self.input_shape,\n",
        "                       dropout=0.5))\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.nb_classes, activation='softmax'))\n",
        "\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mn_ntCtu7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "84ead2e3-8942-4a7b-9cfd-ff54839a4dc2"
      },
      "source": [
        "\"\"\"\n",
        "Train our LSTM on extracted features.\n",
        "\"\"\"\n",
        "% cd /content/geriatrics/Geriatrics_Data/\n",
        "\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "# from models import ResearchModels\n",
        "# from data import DataSet\n",
        "# from extract_features import extract_features\n",
        "import time\n",
        "import os.path\n",
        "import sys\n",
        "\n",
        "def train(data_type, seq_length, model, saved_model=None,\n",
        "          class_limit=None, image_shape=None,\n",
        "          load_to_memory=False, batch_size=32, nb_epoch=100):\n",
        "    # Helper: Save the model.\n",
        "    checkpointer = ModelCheckpoint(\n",
        "        filepath=os.path.join('Geriatrics_Data', 'checkpoints', model + '-' + data_type + \\\n",
        "            '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
        "        verbose=1,\n",
        "        save_best_only=True)\n",
        "\n",
        "    # Helper: TensorBoard\n",
        "    tb = TensorBoard(log_dir=os.path.join('Geriatrics_Data', 'logs', model))\n",
        "\n",
        "    # Helper: Stop when we stop learning.\n",
        "    early_stopper = EarlyStopping(patience=5)\n",
        "\n",
        "    # Helper: Save results.\n",
        "    timestamp = time.time()\n",
        "    csv_logger = CSVLogger(os.path.join('Geriatrics_Data', 'logs', model + '-' + 'training-' + \\\n",
        "        str(timestamp) + '.log'))\n",
        "\n",
        "    # Get the data and process it.\n",
        "    if image_shape is None:\n",
        "        data = DataSet(\n",
        "            seq_length=seq_length,\n",
        "            class_limit=class_limit\n",
        "        )\n",
        "    else:\n",
        "        data = DataSet(\n",
        "            seq_length=seq_length,\n",
        "            class_limit=class_limit,\n",
        "            image_shape=image_shape\n",
        "        )\n",
        "\n",
        "    # Get samples per epoch.\n",
        "    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
        "    steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
        "\n",
        "    if load_to_memory:\n",
        "        # Get data.\n",
        "        X, y = data.get_all_sequences_in_memory('train', data_type)\n",
        "        X_test, y_test = data.get_all_sequences_in_memory('test', data_type)\n",
        "    else:\n",
        "        # Get generators.\n",
        "        generator = data.frame_generator(batch_size, 'train', data_type)\n",
        "        val_generator = data.frame_generator(batch_size, 'test', data_type)\n",
        "\n",
        "    # Get the model.\n",
        "    rm = ResearchModels(len(data.classes), model, seq_length, saved_model)\n",
        "\n",
        "    # Fit!\n",
        "    if load_to_memory:\n",
        "        # Use standard fit.\n",
        "        rm.model.fit(\n",
        "            X,\n",
        "            y,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_test, y_test),\n",
        "            verbose=1,\n",
        "            callbacks=[tb, early_stopper, csv_logger, checkpointer],\n",
        "            epochs=nb_epoch)\n",
        "    else:\n",
        "        # Use fit generator.\n",
        "        rm.model.fit_generator(\n",
        "            generator=generator,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=nb_epoch,\n",
        "            verbose=1,\n",
        "            callbacks=[tb, early_stopper, csv_logger, checkpointer],\n",
        "            validation_data=val_generator,\n",
        "            validation_steps=40,\n",
        "            workers=4)\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"These are the main training settings. Set each before running\n",
        "#     this file.\"\"\"\n",
        "\n",
        "#     if (len(sys.argv) == 5):\n",
        "#         seq_length = int(sys.argv[1])\n",
        "#         class_limit = int(sys.argv[2])\n",
        "#         image_height = int(sys.argv[3])\n",
        "#         image_width = int(sys.argv[4])\n",
        "#     else:\n",
        "#         print (\"Usage: python train.py sequence_length class_limit image_height image_width\")\n",
        "#         print (\"Example: python train.py 75 2 720 1280\")\n",
        "#         exit (1)\n",
        "\n",
        "seq_length = 75\n",
        "class_limit = 2\n",
        "image_height = 240\n",
        "image_width = 640\n",
        "\n",
        "# sequences_dir = os.path.join('Geriatrics_Data', 'sequences')\n",
        "if not os.path.isdir('sequences'):\n",
        "    % mkdir sequences\n",
        "\n",
        "# checkpoints_dir = os.path.join('Geriatrics_Data', 'checkpoints')\n",
        "if not os.path.isdir('checkpoints'):\n",
        "    % mkdir checkpoints\n",
        "\n",
        "# model can be only 'lstm'\n",
        "model = 'lstm'\n",
        "saved_model = None  # None or weights file\n",
        "load_to_memory = False # pre-load the sequences into memory\n",
        "batch_size = 32\n",
        "nb_epoch = 75\n",
        "data_type = 'features'\n",
        "image_shape = (image_height, image_width, 3)\n",
        "\n",
        "% cd /content/geriatrics/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics/Geriatrics_Data\n",
            "/content/geriatrics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZDe6Y3Qt3L1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e99a61a-c7c8-4f8e-e667-8a3527ed6476"
      },
      "source": [
        "nb_epoch = 50\n",
        "extract_features(seq_length=seq_length, class_limit=class_limit, image_shape=image_shape)\n",
        "train(data_type, seq_length, model, saved_model=saved_model, class_limit=class_limit, image_shape=image_shape,load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading LSTM model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 2048)              33562624  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 34,612,738\n",
            "Trainable params: 34,612,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "Creating test generator with 4 samples.\n",
            "Creating train generator with 50 samples.\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.8060 - accuracy: 0.3438 - val_loss: 0.6744 - val_accuracy: 0.5039\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.67438, saving model to Geriatrics_Data/checkpoints/lstm-features.001-0.674.hdf5\n",
            "Epoch 2/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (3.666943). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 6s 6s/step - loss: 0.6334 - accuracy: 0.7188 - val_loss: 0.6835 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.67438\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6418 - accuracy: 0.7188 - val_loss: 0.6703 - val_accuracy: 0.5195\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.67438 to 0.67035, saving model to Geriatrics_Data/checkpoints/lstm-features.003-0.670.hdf5\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6636 - accuracy: 0.5625 - val_loss: 0.6549 - val_accuracy: 0.4930\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.67035 to 0.65490, saving model to Geriatrics_Data/checkpoints/lstm-features.004-0.655.hdf5\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6800 - accuracy: 0.5938 - val_loss: 0.6090 - val_accuracy: 0.4781\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.65490 to 0.60895, saving model to Geriatrics_Data/checkpoints/lstm-features.005-0.609.hdf5\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5803 - accuracy: 0.7500 - val_loss: 0.6082 - val_accuracy: 0.4641\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.60895 to 0.60818, saving model to Geriatrics_Data/checkpoints/lstm-features.006-0.608.hdf5\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5720 - accuracy: 0.8438 - val_loss: 0.6025 - val_accuracy: 0.5109\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.60818 to 0.60252, saving model to Geriatrics_Data/checkpoints/lstm-features.007-0.603.hdf5\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6013 - accuracy: 0.6875 - val_loss: 0.5624 - val_accuracy: 0.5023\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.60252 to 0.56244, saving model to Geriatrics_Data/checkpoints/lstm-features.008-0.562.hdf5\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6075 - accuracy: 0.6875 - val_loss: 0.4955 - val_accuracy: 0.5117\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.56244 to 0.49550, saving model to Geriatrics_Data/checkpoints/lstm-features.009-0.495.hdf5\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6093 - accuracy: 0.6562 - val_loss: 0.5982 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.49550\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5615 - accuracy: 0.7500 - val_loss: 0.4332 - val_accuracy: 0.5078\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.49550 to 0.43319, saving model to Geriatrics_Data/checkpoints/lstm-features.011-0.433.hdf5\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4581 - accuracy: 0.8750 - val_loss: 0.4882 - val_accuracy: 0.7344\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.43319\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5819 - accuracy: 0.6562 - val_loss: 0.3964 - val_accuracy: 0.7578\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.43319 to 0.39640, saving model to Geriatrics_Data/checkpoints/lstm-features.013-0.396.hdf5\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4494 - accuracy: 0.8438 - val_loss: 0.4148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.39640\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5062 - accuracy: 0.7812 - val_loss: 0.3485 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.39640 to 0.34846, saving model to Geriatrics_Data/checkpoints/lstm-features.015-0.348.hdf5\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5065 - accuracy: 0.8125 - val_loss: 0.4020 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.34846\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4882 - accuracy: 0.8125 - val_loss: 0.3762 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.34846\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3967 - accuracy: 0.8125 - val_loss: 0.2697 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.34846 to 0.26975, saving model to Geriatrics_Data/checkpoints/lstm-features.018-0.270.hdf5\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4508 - accuracy: 0.8438 - val_loss: 0.3160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.26975\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4707 - accuracy: 0.9062 - val_loss: 0.2654 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.26975 to 0.26544, saving model to Geriatrics_Data/checkpoints/lstm-features.020-0.265.hdf5\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4856 - accuracy: 0.7812 - val_loss: 0.2940 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.26544\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3920 - accuracy: 0.9062 - val_loss: 0.2623 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.26544 to 0.26228, saving model to Geriatrics_Data/checkpoints/lstm-features.022-0.262.hdf5\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4336 - accuracy: 0.9062 - val_loss: 0.2429 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.26228 to 0.24286, saving model to Geriatrics_Data/checkpoints/lstm-features.023-0.243.hdf5\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4727 - accuracy: 0.7812 - val_loss: 0.2413 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.24286 to 0.24126, saving model to Geriatrics_Data/checkpoints/lstm-features.024-0.241.hdf5\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4160 - accuracy: 0.9062 - val_loss: 0.2127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.24126 to 0.21270, saving model to Geriatrics_Data/checkpoints/lstm-features.025-0.213.hdf5\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3438 - accuracy: 0.9375 - val_loss: 0.2010 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.21270 to 0.20097, saving model to Geriatrics_Data/checkpoints/lstm-features.026-0.201.hdf5\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4111 - accuracy: 0.8438 - val_loss: 0.1770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.20097 to 0.17702, saving model to Geriatrics_Data/checkpoints/lstm-features.027-0.177.hdf5\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3152 - accuracy: 0.9688 - val_loss: 0.1718 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.17702 to 0.17178, saving model to Geriatrics_Data/checkpoints/lstm-features.028-0.172.hdf5\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3311 - accuracy: 1.0000 - val_loss: 0.1708 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.17178 to 0.17083, saving model to Geriatrics_Data/checkpoints/lstm-features.029-0.171.hdf5\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3950 - accuracy: 0.8438 - val_loss: 0.1764 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.17083\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2853 - accuracy: 1.0000 - val_loss: 0.1764 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.17083\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3133 - accuracy: 0.9062 - val_loss: 0.1555 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.17083 to 0.15549, saving model to Geriatrics_Data/checkpoints/lstm-features.032-0.155.hdf5\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3043 - accuracy: 0.9375 - val_loss: 0.1428 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.15549 to 0.14276, saving model to Geriatrics_Data/checkpoints/lstm-features.033-0.143.hdf5\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3159 - accuracy: 0.9688 - val_loss: 0.1662 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.14276\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2787 - accuracy: 0.9062 - val_loss: 0.1461 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.14276\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3125 - accuracy: 0.8750 - val_loss: 0.1237 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.14276 to 0.12367, saving model to Geriatrics_Data/checkpoints/lstm-features.036-0.124.hdf5\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2358 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.12367 to 0.10873, saving model to Geriatrics_Data/checkpoints/lstm-features.037-0.109.hdf5\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3218 - accuracy: 0.8750 - val_loss: 0.1015 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.10873 to 0.10145, saving model to Geriatrics_Data/checkpoints/lstm-features.038-0.101.hdf5\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2618 - accuracy: 1.0000 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.10145\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2576 - accuracy: 0.9375 - val_loss: 0.0902 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.10145 to 0.09021, saving model to Geriatrics_Data/checkpoints/lstm-features.040-0.090.hdf5\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2907 - accuracy: 0.9062 - val_loss: 0.0833 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.09021 to 0.08329, saving model to Geriatrics_Data/checkpoints/lstm-features.041-0.083.hdf5\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2922 - accuracy: 0.9375 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.08329 to 0.07336, saving model to Geriatrics_Data/checkpoints/lstm-features.042-0.073.hdf5\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.07336 to 0.06936, saving model to Geriatrics_Data/checkpoints/lstm-features.043-0.069.hdf5\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2299 - accuracy: 0.9688 - val_loss: 0.0748 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.06936\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2186 - accuracy: 0.9688 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.06936 to 0.06665, saving model to Geriatrics_Data/checkpoints/lstm-features.045-0.067.hdf5\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2445 - accuracy: 0.9375 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.06665 to 0.06510, saving model to Geriatrics_Data/checkpoints/lstm-features.046-0.065.hdf5\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2042 - accuracy: 1.0000 - val_loss: 0.0535 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.06510 to 0.05347, saving model to Geriatrics_Data/checkpoints/lstm-features.047-0.053.hdf5\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.1562 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.05347 to 0.04702, saving model to Geriatrics_Data/checkpoints/lstm-features.048-0.047.hdf5\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2493 - accuracy: 0.9688 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.04702\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2473 - accuracy: 0.9062 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.04702 to 0.04329, saving model to Geriatrics_Data/checkpoints/lstm-features.050-0.043.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2E7_w0-Yhkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "85617b0f-6498-479d-9939-68dfc80a854a"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/geriatrics/Geriatrics_Data/checkpoints/lstm-features.050-0.043.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 47854, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 351, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 724, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 803, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpfqIjARvEMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "92dff852-a40f-4efd-a43a-4221e29dc7e8"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "# from data import DataSet\n",
        "# from extractor import Extractor\n",
        "from keras.models import load_model\n",
        "\n",
        "seq_length = 75\n",
        "class_limit = 2\n",
        "image_height = 240\n",
        "image_width = 640\n",
        "video_file = '/content/geriatrics/Geriatrics_Data/test/Video_Regular_Activity/adl-10-cam0.mp4'\n",
        "saved_model = '/content/geriatrics/Geriatrics_Data/checkpoints/lstm-features.050-0.043.hdf5'\n",
        "\n",
        "capture = cv2.VideoCapture(os.path.join(video_file))\n",
        "width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)   # float\n",
        "height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT) # float\n",
        "% cd /content/geriatrics/\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter(\"result.avi\", fourcc, 15, (int(image_width), int(image_height)))\n",
        "\n",
        "# Get the dataset.\n",
        "data = DataSet(seq_length=seq_length, class_limit=class_limit, image_shape=(image_height, image_width, 3))\n",
        "\n",
        "# get the model.\n",
        "extract_model = Extractor(image_shape=(image_height, image_width, 3))\n",
        "saved_LSTM_model = load_model(saved_model)\n",
        "\n",
        "frames = []\n",
        "frame_count = 0\n",
        "while True:\n",
        "    ret, frame = capture.read()\n",
        "    # Bail out when the video file ends\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save each frame of the video to a list\n",
        "    frame_count += 1\n",
        "    frames.append(frame)\n",
        "\n",
        "    if frame_count < seq_length:\n",
        "        continue # capture frames untill you get the required number for sequence\n",
        "    else:\n",
        "        frame_count = 0\n",
        "\n",
        "    # For each frame extract feature and prepare it for classification\n",
        "    sequence = []\n",
        "    for img in frames:\n",
        "        features = extract_model.extract_image(img)\n",
        "        sequence.append(features)\n",
        "\n",
        "    # Classify sequence\n",
        "    prediction = saved_LSTM_model.predict(np.expand_dims(sequence, axis=0))\n",
        "    print(prediction)\n",
        "    values = data.print_class_from_prediction(np.squeeze(prediction, axis=0))\n",
        "\n",
        "    # Add prediction to frames and write them to new video\n",
        "    for img in frames:\n",
        "        for i in range(len(values)):\n",
        "            cv2.putText(img, values[i], (40, 40 * i + 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), lineType=cv2.LINE_AA)\n",
        "        video_writer.write(img)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "video_writer.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/geriatrics\n",
            "[[0.06043654 0.9395635 ]]\n",
            "Video_Regular_Activity: 0.94\n",
            "Video_Fall: 0.06\n",
            "[[0.05630349 0.9436965 ]]\n",
            "Video_Regular_Activity: 0.94\n",
            "Video_Fall: 0.06\n",
            "[[0.02970673 0.9702932 ]]\n",
            "Video_Regular_Activity: 0.97\n",
            "Video_Fall: 0.03\n",
            "[[0.02645673 0.9735433 ]]\n",
            "Video_Regular_Activity: 0.97\n",
            "Video_Fall: 0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RghWKA2lWl-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DONE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}